{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision \nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T15:18:47.421900Z","iopub.execute_input":"2025-10-31T15:18:47.422146Z","iopub.status.idle":"2025-10-31T15:18:55.532792Z","shell.execute_reply.started":"2025-10-31T15:18:47.422123Z","shell.execute_reply":"2025-10-31T15:18:55.531966Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,)) #mean, std\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T15:18:55.533624Z","iopub.execute_input":"2025-10-31T15:18:55.533985Z","iopub.status.idle":"2025-10-31T15:18:55.537961Z","shell.execute_reply.started":"2025-10-31T15:18:55.533952Z","shell.execute_reply":"2025-10-31T15:18:55.537311Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_dataset = torchvision.datasets.MNIST(root = './data', train=True, download=True, transform = transform)\ntest_dataset = torchvision.datasets.MNIST(root = './data', train=False, download=True, transform = transform)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T15:18:55.539780Z","iopub.execute_input":"2025-10-31T15:18:55.539953Z","iopub.status.idle":"2025-10-31T15:19:00.523013Z","shell.execute_reply.started":"2025-10-31T15:18:55.539940Z","shell.execute_reply":"2025-10-31T15:19:00.521948Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 9.91M/9.91M [00:00<00:00, 12.8MB/s]\n100%|██████████| 28.9k/28.9k [00:00<00:00, 345kB/s]\n100%|██████████| 1.65M/1.65M [00:00<00:00, 3.15MB/s]\n100%|██████████| 4.54k/4.54k [00:00<00:00, 8.61MB/s]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True)\ntest_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T15:19:00.523788Z","iopub.execute_input":"2025-10-31T15:19:00.524080Z","iopub.status.idle":"2025-10-31T15:19:00.528262Z","shell.execute_reply.started":"2025-10-31T15:19:00.524052Z","shell.execute_reply":"2025-10-31T15:19:00.527526Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class MNISTClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = nn.Flatten() #1 channel ->grayscale #flatten Linear Layer from [1,28,25]->[784]\n        self.layers = nn.Sequential(\n            nn.Linear(784, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10)\n        )\n\n    def forward(self, x):\n        x= self.flatten(x)\n        x= self.layers(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T15:21:35.137186Z","iopub.execute_input":"2025-10-31T15:21:35.137797Z","iopub.status.idle":"2025-10-31T15:21:35.142302Z","shell.execute_reply.started":"2025-10-31T15:21:35.137774Z","shell.execute_reply":"2025-10-31T15:21:35.141599Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"#Check for GPU\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Using {device}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T15:21:35.815163Z","iopub.execute_input":"2025-10-31T15:21:35.815610Z","iopub.status.idle":"2025-10-31T15:21:35.819707Z","shell.execute_reply.started":"2025-10-31T15:21:35.815589Z","shell.execute_reply":"2025-10-31T15:21:35.819042Z"}},"outputs":[{"name":"stdout","text":"Using cuda\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Initialize Model and move to device","metadata":{}},{"cell_type":"code","source":"model = MNISTClassifier().to(device)\nprint(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T15:47:03.381317Z","iopub.execute_input":"2025-10-31T15:47:03.381999Z","iopub.status.idle":"2025-10-31T15:47:03.388012Z","shell.execute_reply.started":"2025-10-31T15:47:03.381978Z","shell.execute_reply":"2025-10-31T15:47:03.387436Z"}},"outputs":[{"name":"stdout","text":"MNISTClassifier(\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (layers): Sequential(\n    (0): Linear(in_features=784, out_features=128, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=128, out_features=10, bias=True)\n  )\n)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Loss Function and Optimizer","metadata":{}},{"cell_type":"code","source":"loss_function = nn.CrossEntropyLoss()\nprint(loss_function)\noptimizer = torch.optim.Adam(model.parameters(), lr =0.001)\nprint(optimizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T15:51:00.598138Z","iopub.execute_input":"2025-10-31T15:51:00.598695Z","iopub.status.idle":"2025-10-31T15:51:00.603259Z","shell.execute_reply.started":"2025-10-31T15:51:00.598673Z","shell.execute_reply":"2025-10-31T15:51:00.602481Z"}},"outputs":[{"name":"stdout","text":"CrossEntropyLoss()\nAdam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0\n)\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"def train_epoch(model, train_loader, loss_function, optimizer, device, print_every=50):\n    model.train()\n    running_loss = 0.0\n    running_correct = 0\n    running_total = 0\n\n    num_batches = len(train_loader)\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = loss_function(output, target)\n        loss.backward()\n        optimizer.step()\n\n        # accumulate stats\n        running_loss += loss.item()\n        _, predicted = output.max(1)\n        running_total += target.size(0)\n        running_correct += (predicted == target).sum().item()\n\n        # print every N batches or at the end of the epoch\n        if ((batch_idx + 1) % print_every == 0) or ((batch_idx + 1) == num_batches):\n            avg_loss = running_loss / ((batch_idx % print_every) + 1)\n            accuracy = 100.0 * running_correct / running_total\n            processed = (batch_idx + 1) * data.size(0)\n            total = len(train_loader.dataset)\n            print(f'[{processed}/{total}] Loss: {avg_loss:.3f} | Acc: {accuracy:.1f}%', flush=True)\n\n            # reset window stats\n            running_loss = 0.0\n            running_correct = 0\n            running_total = 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T16:04:46.782007Z","iopub.execute_input":"2025-10-31T16:04:46.782886Z","iopub.status.idle":"2025-10-31T16:04:46.788823Z","shell.execute_reply.started":"2025-10-31T16:04:46.782861Z","shell.execute_reply":"2025-10-31T16:04:46.788239Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"def evaluate(model, test_loader, device):\n    model.eval()\n    correct =0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, targets in test_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            _, predicted = outputs.max(1)\n            total +=targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n    return 100. *correct/total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T16:04:49.241584Z","iopub.execute_input":"2025-10-31T16:04:49.242276Z","iopub.status.idle":"2025-10-31T16:04:49.246869Z","shell.execute_reply.started":"2025-10-31T16:04:49.242248Z","shell.execute_reply":"2025-10-31T16:04:49.245992Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"# Training Loop","metadata":{}},{"cell_type":"code","source":"num_epochs = 10 \nfor epoch in range(num_epochs):\n    print(f'\\nEpoch:{epoch+1}')\n    train_epoch(model, train_loader, loss_function, optimizer, device)\n    accuracy = evaluate(model, test_loader, device)\n    print(f'Test Accuracy:{accuracy:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T16:04:50.860277Z","iopub.execute_input":"2025-10-31T16:04:50.860999Z","iopub.status.idle":"2025-10-31T16:07:06.533326Z","shell.execute_reply.started":"2025-10-31T16:04:50.860977Z","shell.execute_reply":"2025-10-31T16:07:06.532644Z"}},"outputs":[{"name":"stdout","text":"\nEpoch:1\n[3200/60000] Loss: 0.014 | Acc: 99.6%\n[6400/60000] Loss: 0.013 | Acc: 99.7%\n[9600/60000] Loss: 0.013 | Acc: 99.5%\n[12800/60000] Loss: 0.012 | Acc: 99.6%\n[16000/60000] Loss: 0.015 | Acc: 99.4%\n[19200/60000] Loss: 0.014 | Acc: 99.5%\n[22400/60000] Loss: 0.009 | Acc: 99.7%\n[25600/60000] Loss: 0.011 | Acc: 99.6%\n[28800/60000] Loss: 0.011 | Acc: 99.6%\n[32000/60000] Loss: 0.013 | Acc: 99.5%\n[35200/60000] Loss: 0.016 | Acc: 99.4%\n[38400/60000] Loss: 0.014 | Acc: 99.5%\n[41600/60000] Loss: 0.022 | Acc: 99.1%\n[44800/60000] Loss: 0.023 | Acc: 99.1%\n[48000/60000] Loss: 0.023 | Acc: 99.2%\n[51200/60000] Loss: 0.031 | Acc: 99.1%\n[54400/60000] Loss: 0.025 | Acc: 99.1%\n[57600/60000] Loss: 0.020 | Acc: 99.4%\n[30016/60000] Loss: 0.018 | Acc: 99.3%\nTest Accuracy:97.71%\n\nEpoch:2\n[3200/60000] Loss: 0.008 | Acc: 99.8%\n[6400/60000] Loss: 0.009 | Acc: 99.7%\n[9600/60000] Loss: 0.009 | Acc: 99.8%\n[12800/60000] Loss: 0.006 | Acc: 99.9%\n[16000/60000] Loss: 0.006 | Acc: 99.8%\n[19200/60000] Loss: 0.008 | Acc: 99.8%\n[22400/60000] Loss: 0.011 | Acc: 99.7%\n[25600/60000] Loss: 0.018 | Acc: 99.4%\n[28800/60000] Loss: 0.014 | Acc: 99.6%\n[32000/60000] Loss: 0.016 | Acc: 99.4%\n[35200/60000] Loss: 0.021 | Acc: 99.2%\n[38400/60000] Loss: 0.020 | Acc: 99.2%\n[41600/60000] Loss: 0.021 | Acc: 99.1%\n[44800/60000] Loss: 0.020 | Acc: 99.4%\n[48000/60000] Loss: 0.016 | Acc: 99.4%\n[51200/60000] Loss: 0.023 | Acc: 99.0%\n[54400/60000] Loss: 0.025 | Acc: 99.1%\n[57600/60000] Loss: 0.015 | Acc: 99.5%\n[30016/60000] Loss: 0.018 | Acc: 99.4%\nTest Accuracy:97.83%\n\nEpoch:3\n[3200/60000] Loss: 0.013 | Acc: 99.5%\n[6400/60000] Loss: 0.006 | Acc: 99.9%\n[9600/60000] Loss: 0.009 | Acc: 99.7%\n[12800/60000] Loss: 0.019 | Acc: 99.3%\n[16000/60000] Loss: 0.014 | Acc: 99.7%\n[19200/60000] Loss: 0.016 | Acc: 99.4%\n[22400/60000] Loss: 0.017 | Acc: 99.3%\n[25600/60000] Loss: 0.016 | Acc: 99.5%\n[28800/60000] Loss: 0.009 | Acc: 99.8%\n[32000/60000] Loss: 0.013 | Acc: 99.5%\n[35200/60000] Loss: 0.012 | Acc: 99.5%\n[38400/60000] Loss: 0.015 | Acc: 99.4%\n[41600/60000] Loss: 0.014 | Acc: 99.4%\n[44800/60000] Loss: 0.014 | Acc: 99.5%\n[48000/60000] Loss: 0.021 | Acc: 99.3%\n[51200/60000] Loss: 0.013 | Acc: 99.6%\n[54400/60000] Loss: 0.020 | Acc: 99.5%\n[57600/60000] Loss: 0.018 | Acc: 99.3%\n[30016/60000] Loss: 0.013 | Acc: 99.3%\nTest Accuracy:97.85%\n\nEpoch:4\n[3200/60000] Loss: 0.012 | Acc: 99.6%\n[6400/60000] Loss: 0.009 | Acc: 99.6%\n[9600/60000] Loss: 0.007 | Acc: 99.7%\n[12800/60000] Loss: 0.004 | Acc: 99.9%\n[16000/60000] Loss: 0.009 | Acc: 99.7%\n[19200/60000] Loss: 0.010 | Acc: 99.7%\n[22400/60000] Loss: 0.005 | Acc: 99.9%\n[25600/60000] Loss: 0.009 | Acc: 99.8%\n[28800/60000] Loss: 0.007 | Acc: 99.8%\n[32000/60000] Loss: 0.008 | Acc: 99.8%\n[35200/60000] Loss: 0.007 | Acc: 99.8%\n[38400/60000] Loss: 0.009 | Acc: 99.7%\n[41600/60000] Loss: 0.022 | Acc: 99.2%\n[44800/60000] Loss: 0.021 | Acc: 99.3%\n[48000/60000] Loss: 0.019 | Acc: 99.4%\n[51200/60000] Loss: 0.013 | Acc: 99.6%\n[54400/60000] Loss: 0.017 | Acc: 99.5%\n[57600/60000] Loss: 0.014 | Acc: 99.6%\n[30016/60000] Loss: 0.017 | Acc: 99.3%\nTest Accuracy:97.90%\n\nEpoch:5\n[3200/60000] Loss: 0.016 | Acc: 99.5%\n[6400/60000] Loss: 0.014 | Acc: 99.6%\n[9600/60000] Loss: 0.013 | Acc: 99.5%\n[12800/60000] Loss: 0.006 | Acc: 99.8%\n[16000/60000] Loss: 0.014 | Acc: 99.4%\n[19200/60000] Loss: 0.013 | Acc: 99.6%\n[22400/60000] Loss: 0.019 | Acc: 99.4%\n[25600/60000] Loss: 0.016 | Acc: 99.5%\n[28800/60000] Loss: 0.008 | Acc: 99.7%\n[32000/60000] Loss: 0.010 | Acc: 99.7%\n[35200/60000] Loss: 0.009 | Acc: 99.7%\n[38400/60000] Loss: 0.009 | Acc: 99.7%\n[41600/60000] Loss: 0.010 | Acc: 99.6%\n[44800/60000] Loss: 0.012 | Acc: 99.7%\n[48000/60000] Loss: 0.010 | Acc: 99.6%\n[51200/60000] Loss: 0.007 | Acc: 99.8%\n[54400/60000] Loss: 0.012 | Acc: 99.5%\n[57600/60000] Loss: 0.017 | Acc: 99.3%\n[30016/60000] Loss: 0.020 | Acc: 99.3%\nTest Accuracy:98.03%\n\nEpoch:6\n[3200/60000] Loss: 0.008 | Acc: 99.7%\n[6400/60000] Loss: 0.015 | Acc: 99.4%\n[9600/60000] Loss: 0.011 | Acc: 99.6%\n[12800/60000] Loss: 0.005 | Acc: 99.8%\n[16000/60000] Loss: 0.008 | Acc: 99.8%\n[19200/60000] Loss: 0.008 | Acc: 99.8%\n[22400/60000] Loss: 0.005 | Acc: 99.8%\n[25600/60000] Loss: 0.006 | Acc: 99.8%\n[28800/60000] Loss: 0.010 | Acc: 99.6%\n[32000/60000] Loss: 0.008 | Acc: 99.7%\n[35200/60000] Loss: 0.008 | Acc: 99.8%\n[38400/60000] Loss: 0.008 | Acc: 99.7%\n[41600/60000] Loss: 0.005 | Acc: 99.8%\n[44800/60000] Loss: 0.012 | Acc: 99.6%\n[48000/60000] Loss: 0.013 | Acc: 99.5%\n[51200/60000] Loss: 0.010 | Acc: 99.7%\n[54400/60000] Loss: 0.016 | Acc: 99.4%\n[57600/60000] Loss: 0.009 | Acc: 99.6%\n[30016/60000] Loss: 0.014 | Acc: 99.5%\nTest Accuracy:97.66%\n\nEpoch:7\n[3200/60000] Loss: 0.012 | Acc: 99.7%\n[6400/60000] Loss: 0.021 | Acc: 99.3%\n[9600/60000] Loss: 0.013 | Acc: 99.4%\n[12800/60000] Loss: 0.004 | Acc: 99.9%\n[16000/60000] Loss: 0.004 | Acc: 99.9%\n[19200/60000] Loss: 0.005 | Acc: 99.9%\n[22400/60000] Loss: 0.005 | Acc: 99.8%\n[25600/60000] Loss: 0.010 | Acc: 99.7%\n[28800/60000] Loss: 0.017 | Acc: 99.4%\n[32000/60000] Loss: 0.014 | Acc: 99.7%\n[35200/60000] Loss: 0.008 | Acc: 99.7%\n[38400/60000] Loss: 0.011 | Acc: 99.6%\n[41600/60000] Loss: 0.009 | Acc: 99.6%\n[44800/60000] Loss: 0.019 | Acc: 99.4%\n[48000/60000] Loss: 0.026 | Acc: 99.2%\n[51200/60000] Loss: 0.012 | Acc: 99.6%\n[54400/60000] Loss: 0.013 | Acc: 99.6%\n[57600/60000] Loss: 0.017 | Acc: 99.2%\n[30016/60000] Loss: 0.010 | Acc: 99.5%\nTest Accuracy:97.65%\n\nEpoch:8\n[3200/60000] Loss: 0.007 | Acc: 99.7%\n[6400/60000] Loss: 0.007 | Acc: 99.7%\n[9600/60000] Loss: 0.005 | Acc: 99.8%\n[12800/60000] Loss: 0.011 | Acc: 99.6%\n[16000/60000] Loss: 0.007 | Acc: 99.8%\n[19200/60000] Loss: 0.005 | Acc: 99.8%\n[22400/60000] Loss: 0.008 | Acc: 99.7%\n[25600/60000] Loss: 0.010 | Acc: 99.8%\n[28800/60000] Loss: 0.014 | Acc: 99.5%\n[32000/60000] Loss: 0.013 | Acc: 99.5%\n[35200/60000] Loss: 0.011 | Acc: 99.7%\n[38400/60000] Loss: 0.006 | Acc: 99.8%\n[41600/60000] Loss: 0.008 | Acc: 99.8%\n[44800/60000] Loss: 0.010 | Acc: 99.7%\n[48000/60000] Loss: 0.006 | Acc: 99.8%\n[51200/60000] Loss: 0.012 | Acc: 99.6%\n[54400/60000] Loss: 0.013 | Acc: 99.7%\n[57600/60000] Loss: 0.008 | Acc: 99.7%\n[30016/60000] Loss: 0.016 | Acc: 99.5%\nTest Accuracy:97.91%\n\nEpoch:9\n[3200/60000] Loss: 0.007 | Acc: 99.8%\n[6400/60000] Loss: 0.006 | Acc: 99.8%\n[9600/60000] Loss: 0.011 | Acc: 99.7%\n[12800/60000] Loss: 0.010 | Acc: 99.6%\n[16000/60000] Loss: 0.011 | Acc: 99.5%\n[19200/60000] Loss: 0.010 | Acc: 99.8%\n[22400/60000] Loss: 0.015 | Acc: 99.5%\n[25600/60000] Loss: 0.021 | Acc: 99.3%\n[28800/60000] Loss: 0.013 | Acc: 99.4%\n[32000/60000] Loss: 0.025 | Acc: 99.2%\n[35200/60000] Loss: 0.019 | Acc: 99.4%\n[38400/60000] Loss: 0.012 | Acc: 99.4%\n[41600/60000] Loss: 0.011 | Acc: 99.6%\n[44800/60000] Loss: 0.007 | Acc: 99.8%\n[48000/60000] Loss: 0.008 | Acc: 99.6%\n[51200/60000] Loss: 0.006 | Acc: 99.9%\n[54400/60000] Loss: 0.009 | Acc: 99.7%\n[57600/60000] Loss: 0.010 | Acc: 99.6%\n[30016/60000] Loss: 0.007 | Acc: 99.8%\nTest Accuracy:98.00%\n\nEpoch:10\n[3200/60000] Loss: 0.003 | Acc: 99.9%\n[6400/60000] Loss: 0.007 | Acc: 99.7%\n[9600/60000] Loss: 0.009 | Acc: 99.7%\n[12800/60000] Loss: 0.006 | Acc: 99.9%\n[16000/60000] Loss: 0.003 | Acc: 99.9%\n[19200/60000] Loss: 0.005 | Acc: 99.8%\n[22400/60000] Loss: 0.006 | Acc: 99.8%\n[25600/60000] Loss: 0.006 | Acc: 99.8%\n[28800/60000] Loss: 0.010 | Acc: 99.7%\n[32000/60000] Loss: 0.009 | Acc: 99.7%\n[35200/60000] Loss: 0.008 | Acc: 99.7%\n[38400/60000] Loss: 0.005 | Acc: 99.9%\n[41600/60000] Loss: 0.002 | Acc: 100.0%\n[44800/60000] Loss: 0.003 | Acc: 99.9%\n[48000/60000] Loss: 0.007 | Acc: 99.7%\n[51200/60000] Loss: 0.006 | Acc: 99.8%\n[54400/60000] Loss: 0.015 | Acc: 99.6%\n[57600/60000] Loss: 0.007 | Acc: 99.7%\n[30016/60000] Loss: 0.010 | Acc: 99.7%\nTest Accuracy:98.11%\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}